\documentclass[12pt]{article}
\usepackage[top=0.9in, bottom=0.9in, left=0.5in, right=0.5in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{chngcntr}

\setenumerate{parsep=0em, listparindent=\parindent}

% Reset equation numbering for each new item in an enumerate environment.
\makeatletter
\@addtoreset{equation}{enumi}
\makeatother

% Hacky macro to reset equation numbering within sections and subsections. See:
% https://tex.stackexchange.com/questions/264335/when-using-section-counters-dont-reset-properly-how-to-fix-this
\makeatletter
\def\nullstepcounter#1{%
	\begingroup
		\let\@elt\@stpelt
		\csname cl@#1\endcsname
	\endgroup}
\makeatother

\counterwithin*{equation}{section}
\counterwithin*{equation}{subsection}

\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\spn}{span}

\title{Homework 7}
\author{Benjamin Noland}
\date{}

\begin{document}

\maketitle

\section*{Section 5.3}

\begin{enumerate}
\setcounter{enumi}{5}
\item
In any given month, a patient is in precisely one of the following states: ambulatory, bedridden, recovered, or dead. The given data yield the transition matrix
\begin{equation*}
A = \begin{pmatrix}
0.20 & 0.20 & 0 & 0 \\
0.20 & 0.50 & 0 & 0 \\
0.60 & 0.10 & 1 & 0 \\
0 & 0.20 & 0 & 1
\end{pmatrix}
\end{equation*}
and initial probability vector
\begin{equation*}
P = \begin{pmatrix}
0.30 \\
0.70 \\
0 \\
0
\end{pmatrix}.
\end{equation*}
The first entry of $P$ is the initial proportion of ambulatory patients, the second the proportion who are bedridden, the third the proportion who have recovered, and the fourth the proportion who have died.

We compute
\begin{equation*}
AP = \begin{pmatrix}
0.20 & 0.20 & 0 & 0 \\
0.20 & 0.50 & 0 & 0 \\
0.60 & 0.10 & 1 & 0 \\
0 & 0.20 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
0.30 \\
0.70 \\
0 \\
0
\end{pmatrix}
= \begin{pmatrix}
0.20 \\
0.41 \\
0.25 \\
0.14
\end{pmatrix}.
\end{equation*}
Thus after one month of arrival, 20\% of the patients are ambulatory, 41\% are bedridden, 25\% have recovered, and 14\% have died.

Next, we compute
\begin{align*}
\det(tI_4 - A) &= \det\begin{pmatrix}
t - 0.20 & -0.20 & 0 & 0 \\
-0.20 & t - 0.50 & 0 & 0 \\
-0.60 & -0.10 & t - 1 & 0 \\
0 & -0.20 & 0 & t - 1
\end{pmatrix} \\
&= (t - 1) \det\begin{pmatrix}
t - 0.20 & -0.20 & 0 \\
-0.20 & t - 0.50 & 0 \\
-0.60 & -0.10 & t - 1 \\
\end{pmatrix} \\
&= (t - 1)^2 \det\begin{pmatrix}
t - 0.20 & -0.20 \\
-0.20 & t - 0.50 \\
\end{pmatrix} = (t - 1)^2(t - 0.1)(t - 0.6).
\end{align*}
Setting this expression equal to zero and solving for $t$, we find the eigenvalues of $A$ to be $\lambda_1 = 1$ (with multiplicity 2), $\lambda_2 = 0.1$, and $\lambda_3 = 0.6$. We have:
\begin{align*}
B_1 = A - \lambda_1 I_4 &= \begin{pmatrix}
-0.80 & 0.20 & 0 & 0 \\
0.20 & -0.50 & 0 & 0 \\
0.60 & 0.10 & 0 & 0 \\
0 & 0.20 & 0 & 0
\end{pmatrix}
\xrightarrow{\text{Row reduces to}}
\begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{pmatrix} \\
B_2 = A - \lambda_2 I_4 &= \begin{pmatrix}
0.10 & 0.20 & 0 & 0 \\
0.20 & 0.40 & 0 & 0 \\
0.60 & 0.10 & 0.90 & 0 \\
0 & 0.20 & 0 & 0.90
\end{pmatrix}
\xrightarrow{\text{Row reduces to}}
\begin{pmatrix}
1 & 0 & 0 & -9 \\
0 & 1 & 0 & 4.5 \\
0 & 0 & 1 & 5.5 \\
0 & 0 & 0 & 0
\end{pmatrix} \\
B_3 = A - \lambda_3 I_4 &= \begin{pmatrix}
-0.40 & 0.20 & 0 & 0 \\
0.20 & -0.10 & 0 & 0 \\
0.60 & 0.10 & 0.40 & 0 \\
0 & 0.20 & 0 & 0.40
\end{pmatrix}
\xrightarrow{\text{Row reduces to}}
\begin{pmatrix}
1 & 0 & 0 & 1 \\
0 & 1 & 0 & 2 \\
0 & 0 & 1 & -2 \\
0 & 0 & 0 & 0
\end{pmatrix}.
\end{align*}
Thus we find:
\begin{align*}
\ker(B_1) &= \left\{
s \begin{pmatrix}
0 \\
0 \\
1 \\
0
\end{pmatrix} +
t \begin{pmatrix}
0 \\
0 \\
0 \\
1
\end{pmatrix} : s, t \in \mathbb{R}
\right\} \\
\ker(B_2) &= \left\{
s \begin{pmatrix}
18 \\
-9 \\
-11 \\
2
\end{pmatrix} : s \in \mathbb{R}
\right\} \\
\ker(B_3) &= \left\{
s \begin{pmatrix}
-1 \\
-2 \\
2 \\
1
\end{pmatrix} : s \in \mathbb{R}
\right\}.
\end{align*}
For each $1 \leq i \leq 3$, the eigenvalues of $A$ corresponding to $\lambda_i$ are precisely the non-zero elements of $\ker(B_i)$. By part (a) of Thm. 5.9, $A$ is diagonalizable. Thus by part (b) of Thm. 5.9,
\begin{equation*}
\beta' = \left\{
\begin{pmatrix}
0 \\
0 \\
1 \\
0
\end{pmatrix}
\begin{pmatrix}
0 \\
0 \\
0 \\
1
\end{pmatrix},
\begin{pmatrix}
18 \\
-9 \\
-11 \\
2
\end{pmatrix},
\begin{pmatrix}
-1 \\
-2 \\
2 \\
1
\end{pmatrix}
\right\}
\end{equation*}
is an ordered basis for $\mathbb{R}^4$. Consider the linear operator $L_A : \mathbb{R}^4 \to \mathbb{R}^4$. Then
\begin{equation*}
[L_A]_{\beta'} = \begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0.1 & 0 \\
0 & 0 & 0 & 0.6
\end{pmatrix},
\end{equation*}
and by Thm. 2.23, $[L_A]_{\beta'} = Q^{-1} [L_A]_\beta Q$, where $Q$ is the matrix that changes $\beta'$-coordinates into $\beta$-coordinates. Through a simple computation we find that
\begin{equation*}
Q = \begin{pmatrix}
0 & 0 & 18 & -1 \\
0 & 0 & -9 & -2 \\
1 & 0 & -11 & 2 \\
0 & 1 & 2 & 1
\end{pmatrix}
\quad \text{and} \quad
Q^{-1} = \frac{1}{45} \begin{pmatrix}
40 & 25 & 45 & 0 \\
5 & 20 & 0 & 45 \\
2 & -1 & 0 & 0 \\
-9 & -18 & 0 & 0
\end{pmatrix}.
\end{equation*}
Letting $D = [L_A]_{\beta'}$, we have the identity $A = QDQ^{-1}$. Thus, using the Cor. to Thm. 5.12, we get:
\begin{align*}
L &= \lim_{m \to \infty} A^m = \lim_{m \to \infty} (QDQ^{-1})^m = Q (\lim_{m \to \infty} D^m) Q^{-1} \\
&= \begin{pmatrix}
0 & 0 & 18 & -1 \\
0 & 0 & -9 & -2 \\
1 & 0 & -11 & 2 \\
0 & 1 & 2 & 1
\end{pmatrix}
\left[\lim_{m \to \infty} \begin{pmatrix}
1^m & 0 & 0 & 0 \\
0 & 1^m & 0 & 0 \\
0 & 0 & 0.1^m & 0 \\
0 & 0 & 0 & 0.6^m
\end{pmatrix}\right]
\frac{1}{45} \begin{pmatrix}
40 & 25 & 45 & 0 \\
5 & 20 & 0 & 45 \\
2 & -1 & 0 & 0 \\
-9 & -18 & 0 & 0
\end{pmatrix} \\
&= \begin{pmatrix}
0 & 0 & 18 & -1 \\
0 & 0 & -9 & -2 \\
1 & 0 & -11 & 2 \\
0 & 1 & 2 & 1
\end{pmatrix}
\begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{pmatrix}
\frac{1}{45} \begin{pmatrix}
40 & 25 & 45 & 0 \\
5 & 20 & 0 & 45 \\
2 & -1 & 0 & 0 \\
-9 & -18 & 0 & 0
\end{pmatrix} \\
&= \begin{pmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
8/9 & 5/9 & 1 & 0 \\
1/9 & 4/9 & 0 & 1
\end{pmatrix}.
\end{align*}
Therefore:
\begin{equation*}
LP =  \begin{pmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
8/9 & 5/9 & 1 & 0 \\
1/9 & 4/9 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
0.30 \\
0.70 \\
0 \\
0
\end{pmatrix}
= \begin{pmatrix}
0 \\
0 \\
59/90 \\
31/90
\end{pmatrix}.
\end{equation*}
So in the long run 59/90 ($\approx 65.6\%$) of the patients recover, 31/90 ($\approx 34.4\%$) die, and none remain either ambulatory or bedridden.

\end{enumerate}

\section*{Section 5.4}

\begin{enumerate}
\setcounter{enumi}{12}
\item
Note that $W = \spn\{T^k(v) : k \geq 0\}$ by definition. Thus if $w \in W$, then there exist scalars $a_0, \dots, a_n$ satisfying
\begin{equation*}
w = \sum_{i=0}^n a_i T^i(v) = \left(\sum_{i=0}^n a_i T^i \right)(v) = g(T)(v),
\end{equation*}
where $g$ is the polynomial $g(t) = \sum_{i=0}^n a_i t^i$. Conversely, suppose $w \in V$ satisfies $w = g(T)(v)$ for some polynomial $g$. Say $g(t) = \sum_{i=0}^n a_i t^i$. Then
\begin{equation*}
w = g(T)(v) = \left(\sum_{i=0}^n a_i T^i \right)(v) = \sum_{i=0}^n a_i T^i(v),
\end{equation*}
and so we see that $w \in W$.

\setcounter{enumi}{16}
\item
By part (a) of Thm. 5.3, the characteristic polynomial $f$ of $A$ is of the form
\begin{equation*}
f(t) = (-1)^n t^n + a_{n-1} t^{n-1} + \cdots + a_1 t + a_0.
\end{equation*}
By the Cor. to Thm. 5.23,
\begin{equation} \label{eq:1}
O_n = f(A) = (-1)^n A^n + a_{n-1} A^{n-1} + \cdots + a_1 A + a_0 I_n,
\end{equation}
where $O_n$ and $I_n$ denote the $n \times n$ zero and identity matrices, respectively. In particular, from $(\ref{eq:1})$, we see that $A^n$ can be expressed as a linear combination of $I_n, A, \dots, A^{n-1}$; that is, $A^n \in W$, where $W = \spn\{I_n, A, \dots, A^{n-1}\}$. Now let $m > n$, and assume that $A^m \in W$, so that $A^m = b_{n-1}A^{n-1} + \cdots + b_1 A + b_0 I_n$ for some scalars $b_0, \dots, b_{n-1}$. Then
\begin{equation*}
A^{m+1} = A A^m = A(b_{n-1}A^{n-1} + \cdots + b_1 A + b_0 I_n) = b_{n-1}A^{n} + \cdots + b_1 A^2 + b_0 A.
\end{equation*}
Thus since $A^n \in W$, we see that $A^{m+1} \in W$ as well. Therefore $A^m \in W$ for every $m \geq n$ by induction. Thus if $V = \spn\{I_n, A, A^2, \dots\}$, then $V = W$. Since $W$ is finite-dimensional with $\dim(W) \leq n$, we therefore see that $\dim(V) \leq n$, as desired.

\setcounter{enumi}{20}
\item
Suppose that $V$ is not a $T$-cyclic subspace of itself. Let $\{v_1, v_2\}$ be a basis for $V$. Then by assumption, neither $\{v_1, T(v_1)\}$ nor $\{v_2, T(v_2)\}$ spans $V$. Thus $\{v_i, T(v_i)\}$ ($i = 1, 2$) either contains a single element (if $T(v_i) = v_i$), or it contains $2 = \dim(V)$ elements (if $T(v_i) \neq v_i$), in which case it is linearly dependent (by part (b) of Cor. 2 to Thm. 1.10). In either case there exist scalars $c_1$ and $c_2$ with $T(v_1) = c_1 v_1$ and $T(v_2) = c_2 v_2$. By the same sort of argument, there exists a scalar $c$ with $T(v_1 - v_2) = c(v_1 - v_2)$, so that
\begin{equation} \label{eq:1}
c_1 v_1 - c_2 v_2 = T(v_1) - T(v_2) = T(v_1 - v_2) = c(v_1 - v_2) = c v_1 - c v_2.
\end{equation}
Upon rearrangement, $(\ref{eq:1})$ becomes
\begin{equation*}
0 = (c - c_1) v_1 + (c_2 - c) v_2.
\end{equation*}
Thus since $v_1$ and $v_2$ are linearly independent, $c - c_1 = 0$ and $c_2 - c = 0$, so that $c_1 = c_2 = c$. Now let $v \in V$. Then $v = a_1 v_1 + a_2 v_2$ for some scalars $a_1$ and $a_2$. Thus
\begin{equation*}
T(v) = T(a_1 v_1 + a_2 v_2) = a_1 T(v_1) + a_2 T(v_2) = a_1 (c v_1) + a_2 (c v_2) = c(a_1 v_1 + a_2 v_2) = cv.
\end{equation*}
From this we see that $T = cI_V$, where $I_V : V \to V$ denotes the identity linear operator on $V$.

\setcounter{enumi}{26}
\item
\begin{enumerate}
\item
Let $v, v' \in V$ be such that $v + W = v' + W$. This means that $v - v' \in W$. Since $W$ is $T$-invariant, $T(v - v') = T(v) - T(v') \in W$ as well, so that $\overline{T}(v + W) = T(v) + W = T(v') + W = \overline{T}(v' + W)$. Thus $\overline{T}$ is well-defined.

\item
Let $v_1, v_2 \in V$ and let $c$ be a scalar. Then
\begin{align*}
\overline{T}(c(v_1 + W) + (v_2 + W)) &= \overline{T}((c v_1 + v_2) + W) \\
&= T(c v_1 + v_2) + W \\
&= (cT(v_1) + T(v_2)) + W \\
&= c(T(v_1) + W) + (T(v_2) + W) \\
&= c\overline{T}(v_1 + W) + \overline{T}(v_2 + W).
\end{align*}
Thus $\overline{T}$ is a linear operator on $V/W$.

\item
Let $v \in V$. Then
\begin{equation*}
(\eta \circ T)(v) = \eta(T(v)) = T(v) + W = \overline{T}(v + W) = \overline{T}(\eta(v)) = (\overline{T} \circ \eta)(v)
\end{equation*}
Therefore $\eta \circ T = \overline{T} \circ \eta$, as desired.

\end{enumerate}

\setcounter{enumi}{35}
\item
Suppose that $T$ is diagonalizable. Then by Thm. 5.1 there exists an ordered basis $\{w_1, \dots, w_n\}$ for $V$ consisting of eigenvectors of $T$. For each $1 \leq i \leq n$, let $W_i = \spn\{w_i\}$. Then since $w_i \neq 0$, $\{w_i\}$ is a basis for $W_i$. Thus $\dim(W_i) = 1$. Furthermore, note that $T(w_i) = \lambda_i w_i$, where $\lambda_i$ is the eigenvalue of $T$ corresponding to $w_i$. Thus for any $w \in W_i$, since $w = c w_i$ for some scalar $c$,
\begin{equation*}
T(w) = T(cw_i) = cT(w_i) = c(\lambda_i w_i) = \lambda_i (c w_i) = \lambda_i w,
\end{equation*}
so that $T(w) \in W$. Thus $W_i$ is $T$-invariant. Since for each $1 \leq i \leq n$, $\{w_i\}$ is an ordered basis for $W_i$, and $\{w_1\} \cup \cdots \cup \{w_n\} = \{w_1, \dots, w_n\}$ is an ordered basis for $V$, Thm. 5.10 implies that $V = W_1 \oplus \cdots \oplus W_n$.

Conversely, suppose that $V = W_1 \oplus \cdots \oplus W_k$, where for each $1 \leq i \leq k$, $W_i$ is a one-dimensional $T$-invariant subspace of $V$. Note that since $\dim(W_i) = 1$ for each $1 \leq i \leq k$, we must have $k = n$, where $n = \dim(V)$ (see the result of exercise 20 of section 5.2). For each $1 \leq i \leq n$, let $\{w_i\}$ be an ordered basis for $W_i$. Then since $W_i$ is $T$-invariant, $T(w_i) \in W_i$, so that $T(w_i) = \lambda_i w_i$ for some scalar $\lambda_i$. Thus since $w_i \neq 0$, $w_i$ is an eigenvector of $T$. By Thm. 5.10, $\beta = \{w_1\} \cup \cdots \cup \{w_n\} = \{w_1, \dots, w_n\}$ is an ordered basis for $V$. Since $\beta$ consists of eigenvectors of $T$, Thm. 5.1 implies that $T$ is diagonalizable.


\end{enumerate}

\end{document}
